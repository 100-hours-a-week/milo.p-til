## 📅 날짜: 2025-02-10

### 💬 스크럼
- 학습 목표 1 : matplotlib, seaborn, 시계열 데이터 시각화 공부
- 학습 목표 2 : SciPy 및 통계적 시각화 공부
  
### 📒 공부한 내용
---
### 데이터 준비 및 변환

데이터 시각화

- 데이터를 그래프나 차트와 같은 시각적 형식으로 표현하여 패턴, 추세, 인사이트를 효과적으로 전달하는 과정
- 직관적으로 이해하기 쉽게 표현해야 함
- 같은 통계라도 해석하는 방식이 다름, 통찰력을 가져야 함
    - 비행기 폭격 맞고 살아 돌아온 전투기 → 오히려 맞은 부분은 강하다고 해석.  맞고도 돌아왔기 때문
    - 즉 맞은 부분을 보강하는게 아니라 다른 부분을 보강한다는 새로운 아이디어 도출<br><br>

데이터 종류

- 양적 - 수치 (숫자)
- 질적 - 범주 (종류)
- 시계열 - 시간 흐름에 따라 나열
- 지리적 - 위치 기반<br><br>

matplotlib 라이브러리 사용하여 시각화

- label(라벨) 존재<br><br>

---

정형 데이터

- 표나 DB처럼 행과 열의 테이블 구조(=스키마) 를 가진 체계적인 데이터
- DB, excel, CSV, RDBMS(관계형 데이터 저장소) 등에 저장됨
- 저장, 검색, 분석하기 쉽도록 구조화됨
    - 스키마가 명확하므로 메타데이터 기반 구조적 접근이 쉬움<br><br>

스키마

- 데이터 구조, 형식 정의하는 청사진
- 열 이름, 데이터 타입 등 포함하는 설계<br><br>

데이터를 열 기반으로 분석 (위에서 아래로, 즉 어떤 특성 key에 따른 데이터)

- 예 : 이름 특성 데이터들 분석, 나이 특성 데이터들 분석
- 빅데이터 분석에 유리함<br><br>

---

비정형 데이터

- 고정된 스키마(구조)가 없음, 보통 대용량 데이터임, 분석하기 어려움
- 텍스트, 멀티미디어(이미지, 영상, 오디오)
- 파일 시스템, 클라우드, NoSQL DB에서 저장됨(MongoDB 등)<br><br>

반정형 데이터

- JSON, XML 등
- 엄격한 스키마, 구조가 있지는 않지만, 보통 key-value로 구성되며 메타데이터가 존재함<br><br>

비정형 데이터는 전처리 및 변환 과정이 필요함<br><br>

분석과정

1. 수집
    1. 크롤링(페이지 전체 수집), 스크래핑(페이지 일부)
    2. API사용 JSON수집
    3. 파일 읽기
2. 전처리(불필요한 정보 제거)
    1. 예) 텍스트 공백, 특수문자 제거
    2. 예) 이미지 크기 조정, numpy로 변환
3. 정형으로 변환
    1. 예) 텍스트 토큰화 및 감성 분석
    2. 예) 이미지 특징 벡터 추출
4. 분석
    1. 텍스트 데이터 빈도 분석, 이미지 색상 분석 등
5. 시각화<br><br>

### Matplotlib

Matplotlib

- 파이썬에서 다양한 형태의 차트와 그래프를 생성할 수 있도록 지원하는 시각화 라이브러리
- 기본적으로 x축, y축 리스트 데이터가 필요함
- plot 함수를 통해 그래프 생성 가능
- show 함수를 통해 그래프 출력
- kind = ‘bar’, ‘line’ 등으로 그래프 형태 표현 가능
- figsize로 그래프 크기 정함<br><br>

---

막대 그래프

- 범주형 데이터의 단순 크기 비교를 위해 사용
- 각 범주는 개별적인 막대로 표현, 막대 길이는 데이터 크기
- `plt.bar(x, height, color='색상', label='라벨')` 기본 형태
    - 순서대로 범주, 데이터(높이값), 색상, 라벨
    - barh = 가로
    - bottom 지정 시 누적 가능<br><br>

---

히스토그램

- 연속형 데이터의 분포를 나타내기 위해 데이터를 구간(bin)으로 나누고, 각 구간의 빈도수(확률 밀도로 변환 가능)를 시각화
- 즉 개별 데이터를 시각화하는 것이 아닌 구간별로 시각화
    - 예) 영어 점수를 10점 단위 구간으로 나누고 시각화
- 데이터 분포 형태를 직관적으로 파악 가능 - 대칭성, 치우침, 봉우리(피크) 등
- `plt.hist(x, bins='개수', color='색상', edgecolor='색상')`
    - cumulative=True 시 누적 히스토그램
- numpy에 상수값을 더하면 모든 데이터가 그 값만큼 이동
    - `data2 = np.random.randn(1000) + 2`
    - 평균이 0에서 2로 이동한 1000개의 랜덤 데이터<br><br>

random variable(확률 변수) : 함수

- 입력 : 시행의 결과
- 출력 : 실수
- 즉 사건을 숫자로 변환하는 함수. 확률의 개념은 아직 없음<br><br>

확률 분포

- 확률 변수가 취할 수 있는 값들과 각각의 값이 나타날 확률 간의 관계를 매핑하는 함수<br><br>

---

산점도

- 변수 간 관계를 나타내기 위해 각 데이터 점을 좌표 평면에 점으로 표현하는 방법
- 상관관계 확인과 outlier 확인에 좋음, 군집(cluster) 형성 여부 등도 시각적으로 분석 가능, 데이터의 선형, 비선형 파악 좋음
- `plt.scatter(x, y, color='색상', marker='마커')`
- 마커 : 점 모양 (o=원, ^=삼각형 등)<br><br>

카테고리별 색상 구분

```python
import numpy as np  # 수학 연산과 난수 생성을 위한 라이브러리
import matplotlib.pyplot as plt  # 데이터 시각화를 위한 라이브러리

# 데이터 생성
np.random.seed(1)  # 난수 시드 고정
x = np.random.randn(50)  # X축 난수 생성
y = np.random.randn(50)  # Y축 난수 생성
categories = np.random.choice(['A', 'B', 'C'], size=50)  # 'A', 'B', 'C' 중 랜덤 배정
colors = {'A': 'red', 'B': 'blue', 'C': 'green'}  # 카테고리별 색상 지정

# 산점도 생성
for cat in np.unique(categories):  # 유일한 카테고리 목록 순회
    idx = categories == cat  # 특정 카테고리에 해당하는 데이터 선택
    plt.scatter(x[idx], y[idx],  # 해당 카테고리의 x, y 값만 선택하여 표시
                color=colors[cat],  # 지정한 색상 적용
                label=f'Category {cat}',  # 범례 표시
                alpha=0.7,  # 투명도 적용
                s=80)  # 점 크기 설정

plt.xlabel("X")  # X축 라벨
plt.ylabel("Y")  # Y축 라벨
plt.title("Scatter Plot by Category")  # 그래프 제목 설정
plt.legend()  # 범례 추가

# 그래프 표시
plt.show()  # 그래프 출력
```

---

박스플롯(Box Plot)

- 박스 : 데이터의 중간 50%를 나타냄
- 데이터의 분포, 중앙값, 사분위수, outlier 등을 시각적으로 나타내는 방법
    
- 직관적으로 중요한 통계적 특성들을 한눈에 확인 가능함
- 특히 여러 집단의 데이터를 동시 비교할 때 분포 형태를 직관적으로 볼 수 있음
- `plt.boxplot(data)`
- `plt.boxplot([data1, data2, data3])`
- vert = False : 박스 수평 표시<br><br>

---

고급 다중 그래프

- 하나의 figure(장) 안에서 여러 개의 그래프를 동시에 시각화하는 기법
- 비교, 상관관계 파악에 좋음
    - 서로 다른 데이터 간 비교
    - 데이터 흐름과 분포를 동시에 분석
    - 변수 간의 관계 분석
    - 공유된 축을 이용한 데이터 비교
- `plt.subplots()`
- `sharex=True, sharey=Ture` 등 문법으로 축 공유<br><br>

반환값을 분해해서 받는 파이썬 문법

- `fig, axes = plt.subplots(2, 2, figsize=(10, 8))`
- 총 4개의 서브플롯 생성
- `axes[0, 0]` 부터 그래프 생성<br><br>

예시 사용법

```python
fig, axes = plt.subplots(2, 2, figsize=(10, 8))  # 2x2 서브플롯 생성, 크기는 10x8 인치

# 선 그래프
axes[0, 0].plot(x, y1, color='b')  # 첫 번째 그래프 (선 그래프, 파란색)
axes[0, 0].set_title("Line Plot")  # 제목 설정

# 산점도
axes[0, 1].scatter(x, y1, color='r')  # 두 번째 그래프 (산점도, 빨간색)
axes[0, 1].set_title("Scatter Plot")  # 제목 설정

# 막대 그래프
axes[1, 0].bar(np.arange(5), [3, 1, 4, 1, 5], color='g')  # 세 번째 그래프 (막대 그래프, 초록색)
axes[1, 0].set_title("Bar Chart")  # 제목 설정

# 히스토그램
data = np.random.randn(1000)  # 정규 분포를 따르는 난수 1000개 생성
axes[1, 1].hist(data, bins=20, color='purple')  # 네 번째 그래프 (히스토그램, 보라색)
axes[1, 1].set_title("Histogram")  # 제목 설정

# 그래프 출력
plt.tight_layout()  # 그래프 간 간격 자동 조정
plt.show()  # 그래프 화면 출력
```

---

벤 다이어그램

- 여러 집합 간의 관계와 교집합을 시각적으로 표현
- `from matplotlib_venn import venn2`
- `venn2([set_A, set_B])`
- 두 개 집합 비교 시 venn2, 세 개는 venn3 사용<br><br>

파이썬 집합

- 파이썬에서 {} → key, value 형태가 아닌 원소들이면 집합임
- 집합 : 인덱스(순서) 없음, 중복 허용 안됨<br><br>

### Seaborn

Seaborn

- 파이썬 통계적 데이터 시각화 라이브러리
- Matplotlib 기반임, pandas 데이터프레임을 직접 활용 가능함
- 더 직관적이고 보기 좋은 스타일 그래프를 쉽게 생성 가능
- 단순 시각화 말고 통계적 특성 강조
    - 회귀선 추가, 밀도 추정(분포 곡선으로 표현) 등
- 데이터 종류에 초점이 더 맞춰짐 (matplotlib은 그래프에 초점)<br><br>

회귀선 : 두 변수 간의 관계를 수학적 모델(선형 회귀)로 표현한 직선<br><br>


밀도 추정(density estimation) : 데이터 분포를 부드러운 곡선으로 표현, 연속 확률 분포 보여줌<br><br>


사용법

```python
import seaborn as sns  # Seaborn 라이브러리 임포트
import matplotlib.pyplot as plt
```

- `sns.scatterplot() -> plt.show()` seaborn으로 생성 후 출력 예시<br><br>

---

범주형 데이터

- 그룹이나 레이블을 가지는 데이터, 종류형 데이터
- 연속적이지 않고 이산적인 값임
- barplot, boxplot, violinplot, stripplot 등으로 표현
- 예) `sns.barplot(x="Category", y="Value", data=data)`<br><br>

---

연속형 데이터

- x축이 카테고리가 아닌 연속적
- 연속적(무한) 값을 다뤄야 하기 때문에 bin(구간)을 나눠서 빈도로 분석
- 히스토그램, 커널 밀도 추정, 선 그래프, 산점도 등으로 표현<br><br>

KDE(확률 밀도) 추정 → seaborn에서 선으로 표현해줌

- `kde=True` 를 사용

회귀선 포함 시각화

- `regplot()` → 회귀선 포함 산점도 생성
    

    

---

관계 데이터

- 두 개 이상의 변수 간의 상관관계나 패턴을 분석하는 데이터
- 수치형 데이터간의 상관관계 분석에 활용
    - 인과관계 : 원인(우유) - 결과(키 성장) ⇒ 가장 좋으나 찾기 힘듦
    - 상관관계 : 두 사건의 관련성 ⇒ 관계를 찾아낸것 자체가 중요함
        - 예) 소비 증가와 매출의 관계
        - 예) 운동량과 체중 감소 간의 관계
- `sns.pairplot(data, vars=['column', 'column', 'column'])`
    - var을 통해 분석할 변수들 지정<br><br>

### 시계열 데이터 시각화

시계열 데이터

- 시간 흐름에 따라 일정한 간격(초,분,시간 등)으로 측정된 연속적 데이터
- 센서 값인 경우가 많음(날씨, IoT센서, 주식, 트래픽 등)
- 시간에 따른 변화를 분석해 미래 예측을 위해 사용함<br><br>

이동 평균 : 폭을 옮겨가면서 평균 계산

- window : 폭
- 예 ) 7일 평균<br><br>

이상치(outlier) 탐지

- 보통 전체 데이터의 75%안에 없다 : outlier<br><br>

예시 중요 코드

- `np.cumsum(np.random.randn(100))`: 100개의 랜덤 데이터를 생성하고 누적합을 적용하여 시계열 데이터처럼 만듦
- `df["Moving_Avg"] = df["Value"].rolling(window=7).mean()`: 7일 이동 평균을 계산하여 새로운 컬럼 `Moving_Avg`에 저장
    
    ```python
    # 이동 평균(7일 평균) 계산
    df["Moving_Avg"] = df["Value"].rolling(window=7).mean()  # 7일 이동 평균 계산
    
    # 이동 평균 그래프 시각화
    plt.figure(figsize=(10, 5))
    sns.lineplot(x="Date", y="Value", data=df, label="Original Data", color="gray")  # 원본 데이터
    sns.lineplot(x="Date", y="Moving_Avg", data=df, label="7-Day Moving Average", color="red")  # 이동 평균
    
    plt.xlabel("Date")
    plt.ylabel("Value")
    plt.title("Time Series with Moving Average")
    plt.legend()  # 범례 추가
    plt.xticks(rotation=45)
    plt.show()
    ```
    

---

ai와의 연관성

- 입력과 출력 : ML/DL 모델은 입력 데이터를 받아 학습을 진행한 후 출력 데이터, 예측 결과를 내놓음
    - 입력 데이터 : 원천 정보, 텍스트, 이미지, 시계열 데이터 등등..
    - 출력 데이터 : 분류, 회귀, 생성 등 다양한 형태 결과 도출
- 데이터 중요성 (좋은 데이터를 넣는 법)
    - 학습, 검증, 테스트 데이터셋이 필요함
    - 데이터 품질이 좋고 양이 많아야 함, 그러나 실제 데이터는 너무 부족함
- 데이터 확충
    - 데이터 증강, 뻥튀기 : 회전, 크기조정, 노이즈 추가 등 변형하여 인위적으로 늘림
    - 샘플링 : 기존 데이터를 적절히 선택 혹은 재구성
    - k-fold 교차검증 - 여러 폴드로 나눠서 각 폴드마다 학습과 검증 반복
    - 혹은 인공적으로 생성한 합성 데이터를 추가하는 방법으로 해결함<br><br>

합성 데이터

- 실제 생성된 데이터가 아닌 인공적으로 생성한 데이터
- 데이터가 너무 부족하면 합성이 필요하다.<br><br>

sampling : 추출

- 표본 평균 : 모집단의 모평균과 대비됨. 모집단에서 샘플 추출 후 그것들의 평균<br><br>

resampling : 리샘플링

- 데이터 시간 간격 재조정. 데이터 변환 과정
- 다운샘플링 : 데이터 양 축소. 즉 더 긴 시간 간격으로 데이터 집계 → 평균 혹은 중앙값 등 함수 사용하여 데이터 요약함
- 업샘플링 : 데이터 양 증가, 더 짧은 시간 간격으로 데이터 집계 → 결측값 보간 기법으로 증가시키며 누락된 값 보충
    - NaN값 Interpolation(보간)
        
        
        | 보간법 | 사용법 | 설명 | 특징 |
        | --- | --- | --- | --- |
        | **선형 보간
        (Linear Interpolation)** | `df.interpolate(method="linear")` | 두 인접한 값 사이를 직선으로 연결하여 `NaN` 값을 채움 | 데이터가 **일정한 증가/감소 경향**을 가질 때 유용 |
        | **전방 채우기
        (Forward Fill, ffill)** | `df.interpolate(method="ffill")` 또는 `df.fillna(method="ffill")` | `NaN` 값을 바로 이전 값으로 채움 | **이전 값이 유지되는 데이터** (예: 센서 데이터, 주식 가격)에서 사용 |
        | **후방 채우기
        (Backward Fill, bfill)** | `df.interpolate(method="bfill")` 또는 `df.fillna(method="bfill")` | `NaN` 값을 바로 다음 값으로 채움 | **빠른 응답이 필요한 데이터**에서 사용 |
        | **다항식 보간
        (Polynomial Interpolation)** | `df.interpolate(method="polynomial", order=n)` | 다항식 회귀를 이용하여 보간 | 곡선형 패턴이 있는 경우 적합하지만, **과적합 위험** 존재 |
        | **스플라인 보간
        (Spline Interpolation)** | `df.interpolate(method="spline", order=n)` | 스플라인 보간법을 이용하여 부드러운 곡선으로 채움 | **곡선형 패턴이 자연스러워야 할 때** 유용 |
        | **시간 보간
        (Time Interpolation)** | `df.interpolate(method="time")` | 시간 간격을 고려하여 보간 | **시계열 데이터에서 시간 흐름을 유지**할 때 유용 |
        | **제곱 보간
        (Quadratic Interpolation)** | `df.interpolate(method="quadratic")` | 2차 곡선(포물선) 형태로 보간 | 데이터가 **곡선 형태로 변화**하는 경우 적합 |
- 불규칙한 시계열 데이터를 일정 간격으로 정리하기 위함
- 변동성 완화하고 노이즈 제거하기 위함

---

이동 평균

- 시계열 데이터의 변동성을 완화하고 장기적인 추세를 파악하기 위해 일정 구간의 평균값을 계산하는 기법
- 원본데이터를 고정 크기의 window로 일정량씩 묶어서 평균을 낸 것<br><br>

이동 평균 사용 이유

- 변동성(noise) 감소 : 데이터가 확 튀거나 이상한 노이즈값이 반영되면 데이터의 핵심을 못 볼수도 있음. 이런 변동을 부드럽게 해주는 역할을 함
- 장기적 트렌드 분석에 유리 : 데이터를 부드럽게 조정해 장기적 흐름 강조
- 이상치 탐지에 유리함 : 이동평균과 원본 비교 시 쉽게 찾을 수 있음
- 예측 모델링에서 활용 : ML에서 중요한 전처리 방법임. 좋은 입력 데이터를 생성하기 때문<br><br>

pandas 함수 사용

- 단순 이동평균 - `df["SMA"] = df["value"].rolling(window=윈도우_크기).mean()`
- 지수 이동평균 - `df["EMA"] = df["value"].ewm(span=윈도우_크기, adjust=False).mean()`
- 가중 이동평균
    
    ```python
    weights = np.arange(1, 윈도우_크기 + 1)  # 가중치 배열 생성
    df["WMA"] = df["value"].rolling(window=윈도우_크기).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)
    ```
    
    | 매개변수 | 설명 |
    | --- | --- |
    | `np.arange(1, 윈도우_크기 + 1)` | 최근 값일수록 더 높은 가중치를 가지도록 1부터 증가하는 배열 생성 |
    | `rolling(window=윈도우_크기)` | 이동 윈도우를 설정하여 가중 평균 적용 |
    | `.apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)` | 가중 평균을 직접 계산하는 함수 적용 |
    | `np.dot(x, weights) / weights.sum()` | 가중 평균을 계산하여 반영 |

---

금융 데이터

- 주식 가격, 거래량, 환율, 금리 등 금융 시장에서 발생하는 시계열 기반의 데이터
- 가격(시가, 고가, 종가 등), 거래량, 지수, 환율, 금리
- 최적의 의사결정을 내리기 위해 사용<br><br>

### SciPy 및 통계적 시각화

Scipy(scientific python)

- numpy 기반, 과학계산, 통계분석을 위한 함수와 알고리즘 제공하는 라이브러리
- 최적화(optimization) - 함수 최솟값 구하기 가능 `scipy.optimize`
- 통계 분석 - 정규분포 생성 및 분석 `scipy.stats`
- 신호 처리 - FFT, 고속 푸리에 변환 함수 제공 `scipy.fft`
- 적분 - 정적분 계산 제공 `scipy.integrate`
- 선형대수 - 고유값 고유벡터 (선형 변환 성질) `scipy.linalg`<br><br>

---

정규 분포 - 데이터가 평균을 중심으로 좌우대칭을 이루는 확률분포

- 많은 데이터는 정규분포를 따름 (자연현상, 사회적 데이터 등)

정규 분포는 확률밀도함수(PDF)로 정의됨

- `μ` 와`σ` (평균, 표준편차)

- `np.random.normal` 로 생성
- `sns.histplot` 으로 PDF 시각화
- `stats.norm.pdf` 로 특정 값 확률 밀도 계산<br><br>

---

기술 통계

- 데이터를 요약하고 정리하여 평균, 중앙값, 표준 편차 등과 같은 통계적 지표를 계산하고 시각적으로 표현하는 분석 기법
- 중앙 경향성(Central Tendency), 산포도(Dispersion), 분포 특성(Distribution Characteristics) 세 가지 영역으로 나눌 수 있음<br><br>

1. 중앙 경향성
    - 평균, 중앙값, 최빈값
2. 산포도
    - 범위(최대,최소 차이)
    - 분산
    - 표준편차
    - IQR(사분위 범위) - 1사분위수, 3사분위수의 차이. 데이터 중간 영역의 퍼짐 정도
3. 분포 특성
    - 왜도(Skewness) - 분포의 비대칭성 값(오른쪽, 왼쪽 치우침)
    - 첨도(Kurtosis) - 분포의 뾰족한 정도(3보다 크면 뾰족, 3보다 작으면 완만)<br><br>

---

가설 검정 - 표본을 기반으로 통계적 가설의 참과 거짓을 검정하여 결론 도출하는 과정. 즉 그 가설이 우연이 아니라 실제로 차이가 존재하는지를 검정<br><br>

- 귀무가설 - 기본적으로 참이라고 생각함   예) 차이가 없다
- 대립가설 - 연구자가 증명해야 할, 검정해야할 가정   예) 차이가 있다
- Type1 error - 귀무가설이 참인데, 결론을 잘못 내림 (차이가 없는데 있다고 착각)
- Type2 error - 대립가설이 참인데, 귀무가설 받아들임 (차이가 실제로 있는데 없다고-우연이었다고 착각)
- critical value - 임계값. 초과하면 귀무가설 기각(실제로 효과있음), 초과하지 않으면 그냥 귀무가설 (우연) → 유의수준 alpha에서 임계값 결정
- p-value - 귀무가설이 참일 때 극단적인 결과가 나올 확률
    - 즉 p-value가 critical value보다 작으면 대립가설 채택<br><br>

기본 문법

| **검정 방법** | **기본 문법** | **설명** |
| --- | --- | --- |
| **단일 표본 t-검정 (One-Sample t-test)** | `stats.ttest_1samp(data, popmean)` | 한 그룹의 평균이 특정 값과 유의미한 차이가 있는지 검정 |
| **독립 표본 t-검정 (Independent t-test)** | `stats.ttest_ind(sample1, sample2)` | 두 개의 독립적인 그룹의 평균 차이를 검정 |
| **대응 표본 t-검정 (Paired t-test)** | `stats.ttest_rel(before, after)` | 같은 그룹의 두 시점 간 차이를 검정 |
| **카이제곱 검정 (Chi-Square Test)** | `stats.chisquare(observed, expected)` | 범주형 데이터의 빈도 분포 차이를 검정 |
| **ANOVA (분산 분석)** | `stats.f_oneway(group1, group2, group3)` | 세 개 이상의 그룹 간 평균 차이를 검정 |
| **회귀 분석 (Regression Analysis)** | `stats.linregress(x, y)` | 변수 X가 변수 Y에 미치는 영향을 검정 |

---

통계적 시각화

- 데이터의 분포, 관계, 추세 등을 효과적으로 분석하기 위해 통계적 기법을 활용하여 그래프나 차트로 표현하는 과정
- 단순한 시각화와 달리 통계적인 요약 및 분석을 포함하여 보다 깊이 있게 탐색<br><br>

통계적 시각화 분석 종류

| **구분** | **설명** | **예제 시각화** | 사용 예시 |
| --- | --- | --- | --- |
| **단변량 분석 (Univariate Analysis)** | 하나의 변수에 대한 데이터 분포 및 특성을 분석 | 히스토그램, 커널 밀도 추정(KDE), 박스플롯 | 학생들의 시험 점수 분포를 확인하기 위해 **히스토그램**을 사용 |
| **이변량 분석 (Bivariate Analysis)** | 두 변수 간의 관계를 분석 | 산점도, 상관 행렬, 회귀선 그래프 | 공부 시간과 성적 간 관계를 파악하기 위해 **산점도**를 사용 |
| **다변량 분석 (Multivariate Analysis)** | 세 개 이상의 변수 간 관계를 분석 | 페어플롯, 3D 산점도, 히트맵 | 여러 변수 간 연관성을 한눈에 보기 위해 **히트맵**을 사용 |

SciPy + Seaborn 사용해서 가설 검정 후 시각화도 가능

### 한 줄 정리

| 개념 | 정리 |
| --- | --- |
| 시각화 (Visualization) | 데이터를 그래프, 차트, 히스토그램 등 시각적 형식으로 표현하여 직관적이고 이해하기 쉽도록 정보를 전달하는 방법이다. |
| 정형 데이터 (Structured Data) | 표, DB처럼 행과 열의 스키마를 가진 체계적인 데이터로 저장, 검색, 분석하기에 쉽다는 장점이 있다 |
| 비정형 데이터 (Unstructured Data) | 고정된 스키마가 없는 텍스트, 멀티미디어(이미지, 영상, 오디오) 데이터로 보통 대용량 데이터이며 정형 데이터보다 분석하기 어렵다 |
| 히스토그램 (Histogram) | 연속형 데이터 분포를 나타내기 위해 개별 데이터가 아닌 구간으로 나눠 빈도수를 시각화 한 방식으로 데이터 분포 형태를 직관적으로 파악 가능하다는 장점이 있다. |
| 박스 플롯 (Box Plot) | 데이터의 중간 50%를 나타내는 박스를 통해 분포, 중앙값, 사분위수와 outlier 등을 시각적으로 나타낸 방법이다. |
| 범주형 데이터 (Categorical Data) | 그룹이나 레이블을 가지는 종류 데이터를 말하며 연속적이지 않고 이산적인 값을 가진다. |
| 연속형 데이터 (Continuous Data) | 숫자나 측정값처럼 연속적으로 존재하는 데이터를 말하며 특정 구간 내에서 모든 값이 가능하기 때문에 구간을 나눠 빈도로 분석한다. |
| 시계열 데이터 (Time Series Data) | 시간 흐름에 따라 초, 분, 시간 등 일정한 간격으로 센서에서 측정된 연속적 데이터이며 시간별 변화를 분석해 미래 예측에 사용된다. |
| 이동 평균 (Moving Average) | 시계열 데이터에서 일정 기간 동안의 데이터 값들의 평균을 계산하는 방법으로 노이즈와 이상치를 줄여 단기 변동성을 줄이고 장기 추세를 파악할 수 있다. |



---
  
### 📁 참고 자료 및 링크
- alex 강의
